{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import os\n",
    "import utils_dir.langchain_utils as langchain_utils"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T14:02:33.715864600Z",
     "start_time": "2023-10-16T14:02:32.554453400Z"
    }
   },
   "id": "824b1fb2415e7ff7"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-16T13:28:14.877546400Z",
     "start_time": "2023-10-16T13:28:14.868496100Z"
    }
   },
   "outputs": [],
   "source": [
    "docs_dir = '../docs_loka_1doc'\n",
    "collection_path = \"D:\\python_projects\\loka_final\\langchain_embed_llama2_db_chroma_qa_llama2/db\"\n",
    "collection_name = \"loka_aws\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilija\\AppData\\Local\\Temp\\ipykernel_18412\\2228210933.py:22: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  text = ''.join(soup.findAll(text=True))\n"
     ]
    }
   ],
   "source": [
    "documents = langchain_utils.read_documents(docs_dir = docs_dir)\n",
    "\n",
    "texts = langchain_utils.split_documents(documents = documents)\n",
    "\n",
    "embeddings = langchain_utils.get_embeddings_object(model_name = 'llamacpp')\n",
    "\n",
    "langchain_utils.check_langchain_gpu_usage()\n",
    "\n",
    "persist_directory = f'./db_{os.path.basename(os.path.normpath(docs_dir))}'\n",
    "docsearch = langchain_utils.load_chroma(texts = texts,\n",
    "                                        embeddings = embeddings,\n",
    "                                        persist_directory = persist_directory)\n",
    "\n",
    "llm = langchain_utils.get_llm_object('llamacpp')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T13:28:15.651320300Z",
     "start_time": "2023-10-16T13:28:15.581408500Z"
    }
   },
   "id": "bd8110f41e6d5e0e"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SageMaker is a fully managed machine learning service that helps you quickly build, train, and deploy predictive models at scale with ease, without the need to build or maintain infrastructure. It's easy to get started with training, hosting, and serving your model in production using Amazon Simple Storage Service (Amazon S3).\n",
      "\n",
      "\n",
      "[Document(page_content='/opt/ml\\n├── input\\n│   ├── config\\n│   │   ├── hyperparameters.json\\n│   │   └── resourceConfig.json\\n│   └── data\\n│       └── <channel_name>\\n│           └── <input data>\\n├── model\\n│\\n├── code\\n│\\n├── output\\n│\\n└── failure\\nWhen you run a model training job, the SageMaker container uses the /opt/ml/input/ directory, which contains the JSON files that configure the hyperparameters for the algorithm and the network layout used for distributed training. The /opt/ml/input/ directory also contains files that specify the channels through which SageMaker accesses the data, which is stored in Amazon Simple Storage Service (Amazon S3). The SageMaker containers library places the scripts that the container will run in the /opt/ml/code/ directory. Your script should write the model generated by your algorithm to the /opt/ml/model/ directory. For more information, see Use Your Own Training Algorithms.', metadata={'source': '..\\\\docs_loka_1doc\\\\sagemaker_documentation\\\\amazon-sagemaker-toolkits.md'}), Document(page_content='When you host a trained model on SageMaker to make inferences, you deploy the model to an HTTP endpoint. The model makes real-time predictions in response to inference requests. The container must contain a serving stack to process these requests.\\nIn a hosting or batch transform container, the model files are located in the same folder to which they were written during training.\\n/opt/ml/model\\n│\\n└── <model files>\\nFor more information, see Use your own inference code.\\nSingle Versus Multiple Containers\\nYou can either provide separate Docker images for the training algorithm and inference code or you can use a single Docker image for both. When creating Docker images for use with SageMaker, consider the following:\\n+ Providing two Docker images can increase storage requirements and cost because common libraries might be duplicated.', metadata={'source': '..\\\\docs_loka_1doc\\\\sagemaker_documentation\\\\amazon-sagemaker-toolkits.md'}), Document(page_content='+ Providing two Docker images can increase storage requirements and cost because common libraries might be duplicated.\\n+ In general, smaller containers start faster for both training and hosting. Models train faster and the hosting service can react to increases in traffic by automatically scaling more quickly.\\n+ You might be able to write an inference container that is significantly smaller than the training container. This is especially common when you use GPUs for training, but your inference code is optimized for CPUs.\\n+ SageMaker requires that Docker containers run without privileged access.\\n+ Both Docker containers that you build and those provided by SageMaker can send messages to the Stdout and Stderr files. SageMaker sends these messages to Amazon CloudWatch logs in your AWS account.', metadata={'source': '..\\\\docs_loka_1doc\\\\sagemaker_documentation\\\\amazon-sagemaker-toolkits.md'}), Document(page_content='+ Both Docker containers that you build and those provided by SageMaker can send messages to the Stdout and Stderr files. SageMaker sends these messages to Amazon CloudWatch logs in your AWS account.\\nFor more information about how to create SageMaker containers and how scripts are executed inside them, see the SageMaker Training Toolkit and SageMaker Inference Toolkit repositories on GitHub. They also provide lists of important environmental variables and the environmental variables provided by SageMaker containers.', metadata={'source': '..\\\\docs_loka_1doc\\\\sagemaker_documentation\\\\amazon-sagemaker-toolkits.md'})]\n"
     ]
    }
   ],
   "source": [
    "qa = langchain_utils.get_retrieval_qa(llm = llm,\n",
    "                                      vector_db = docsearch)\n",
    "result = qa({\"query\":\"What is Sagemaker?\"})\n",
    "print(result[\"result\"]) \n",
    "print(result[\"source_documents\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T13:33:33.894121200Z",
     "start_time": "2023-10-16T13:28:29.457814500Z"
    }
   },
   "id": "8cccf5b3c5762ff1"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Amazon SageMaker is a fully-managed service for building, training, and deploying machine learning (ML) models quickly and securely on cloud infrastructure. This provides you the flexibility to build ML models from within your applications or at the command line to make predictions in real time. With SageMaker, you can develop complex deep learning models using popular frameworks such as PyTorch and TensorFlow in your local IDEs.\n",
      "\n",
      "SageMaker is a managed service that allows you to rapidly build, train, and deploy machine learning (ML) models with just a few simple commands. This allows you to focus on the algorithms themselves rather than the underlying infrastructure required to train the model. The SageMaker training API allows you to create training jobs in just a few lines of code by specifying the data sources for the training job and which algorithm you want to use. You can also specify custom hyperparameters for your ML models.\n",
      "To learn more about how Amazon SageMaker works, see How It Works.\n",
      "To get started with using Amazon SageMaker, see Using Amazon SageMaker in the AWS Management Console. You may also choose to work through the tutorials and code samples on Github. Finally,\n",
      "[Document(page_content='/opt/ml\\n├── input\\n│   ├── config\\n│   │   ├── hyperparameters.json\\n│   │   └── resourceConfig.json\\n│   └── data\\n│       └── <channel_name>\\n│           └── <input data>\\n├── model\\n│\\n├── code\\n│\\n├── output\\n│\\n└── failure\\nWhen you run a model training job, the SageMaker container uses the /opt/ml/input/ directory, which contains the JSON files that configure the hyperparameters for the algorithm and the network layout used for distributed training. The /opt/ml/input/ directory also contains files that specify the channels through which SageMaker accesses the data, which is stored in Amazon Simple Storage Service (Amazon S3). The SageMaker containers library places the scripts that the container will run in the /opt/ml/code/ directory. Your script should write the model generated by your algorithm to the /opt/ml/model/ directory. For more information, see Use Your Own Training Algorithms.', metadata={'source': '..\\\\docs_loka_1doc\\\\sagemaker_documentation\\\\amazon-sagemaker-toolkits.md'}), Document(page_content='When you host a trained model on SageMaker to make inferences, you deploy the model to an HTTP endpoint. The model makes real-time predictions in response to inference requests. The container must contain a serving stack to process these requests.\\nIn a hosting or batch transform container, the model files are located in the same folder to which they were written during training.\\n/opt/ml/model\\n│\\n└── <model files>\\nFor more information, see Use your own inference code.\\nSingle Versus Multiple Containers\\nYou can either provide separate Docker images for the training algorithm and inference code or you can use a single Docker image for both. When creating Docker images for use with SageMaker, consider the following:\\n+ Providing two Docker images can increase storage requirements and cost because common libraries might be duplicated.', metadata={'source': '..\\\\docs_loka_1doc\\\\sagemaker_documentation\\\\amazon-sagemaker-toolkits.md'}), Document(page_content='+ Providing two Docker images can increase storage requirements and cost because common libraries might be duplicated.\\n+ In general, smaller containers start faster for both training and hosting. Models train faster and the hosting service can react to increases in traffic by automatically scaling more quickly.\\n+ You might be able to write an inference container that is significantly smaller than the training container. This is especially common when you use GPUs for training, but your inference code is optimized for CPUs.\\n+ SageMaker requires that Docker containers run without privileged access.\\n+ Both Docker containers that you build and those provided by SageMaker can send messages to the Stdout and Stderr files. SageMaker sends these messages to Amazon CloudWatch logs in your AWS account.', metadata={'source': '..\\\\docs_loka_1doc\\\\sagemaker_documentation\\\\amazon-sagemaker-toolkits.md'}), Document(page_content='+ Both Docker containers that you build and those provided by SageMaker can send messages to the Stdout and Stderr files. SageMaker sends these messages to Amazon CloudWatch logs in your AWS account.\\nFor more information about how to create SageMaker containers and how scripts are executed inside them, see the SageMaker Training Toolkit and SageMaker Inference Toolkit repositories on GitHub. They also provide lists of important environmental variables and the environmental variables provided by SageMaker containers.', metadata={'source': '..\\\\docs_loka_1doc\\\\sagemaker_documentation\\\\amazon-sagemaker-toolkits.md'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \\\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "qa = langchain_utils.get_retrieval_qa(llm = llm,\n",
    "                                      vector_db = docsearch,\n",
    "                                      prompt = prompt)\n",
    "\n",
    "result2 = qa({\"query\":\"What is Sagemaker?\"})\n",
    "print(result2[\"result\"]) \n",
    "print(result2[\"source_documents\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T13:35:22.963279900Z",
     "start_time": "2023-10-16T13:33:33.964536500Z"
    }
   },
   "id": "24c152ad00a4de"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Amazon SageMaker is a fully managed platform for building, training, and hosting machine learning (ML) models securely and at scale.\n",
      "[Document(page_content='/opt/ml\\n├── input\\n│   ├── config\\n│   │   ├── hyperparameters.json\\n│   │   └── resourceConfig.json\\n│   └── data\\n│       └── <channel_name>\\n│           └── <input data>\\n├── model\\n│\\n├── code\\n│\\n├── output\\n│\\n└── failure\\nWhen you run a model training job, the SageMaker container uses the /opt/ml/input/ directory, which contains the JSON files that configure the hyperparameters for the algorithm and the network layout used for distributed training. The /opt/ml/input/ directory also contains files that specify the channels through which SageMaker accesses the data, which is stored in Amazon Simple Storage Service (Amazon S3). The SageMaker containers library places the scripts that the container will run in the /opt/ml/code/ directory. Your script should write the model generated by your algorithm to the /opt/ml/model/ directory. For more information, see Use Your Own Training Algorithms.', metadata={'source': '..\\\\docs_loka_1doc\\\\sagemaker_documentation\\\\amazon-sagemaker-toolkits.md'}), Document(page_content='When you host a trained model on SageMaker to make inferences, you deploy the model to an HTTP endpoint. The model makes real-time predictions in response to inference requests. The container must contain a serving stack to process these requests.\\nIn a hosting or batch transform container, the model files are located in the same folder to which they were written during training.\\n/opt/ml/model\\n│\\n└── <model files>\\nFor more information, see Use your own inference code.\\nSingle Versus Multiple Containers\\nYou can either provide separate Docker images for the training algorithm and inference code or you can use a single Docker image for both. When creating Docker images for use with SageMaker, consider the following:\\n+ Providing two Docker images can increase storage requirements and cost because common libraries might be duplicated.', metadata={'source': '..\\\\docs_loka_1doc\\\\sagemaker_documentation\\\\amazon-sagemaker-toolkits.md'}), Document(page_content='+ Providing two Docker images can increase storage requirements and cost because common libraries might be duplicated.\\n+ In general, smaller containers start faster for both training and hosting. Models train faster and the hosting service can react to increases in traffic by automatically scaling more quickly.\\n+ You might be able to write an inference container that is significantly smaller than the training container. This is especially common when you use GPUs for training, but your inference code is optimized for CPUs.\\n+ SageMaker requires that Docker containers run without privileged access.\\n+ Both Docker containers that you build and those provided by SageMaker can send messages to the Stdout and Stderr files. SageMaker sends these messages to Amazon CloudWatch logs in your AWS account.', metadata={'source': '..\\\\docs_loka_1doc\\\\sagemaker_documentation\\\\amazon-sagemaker-toolkits.md'}), Document(page_content='+ Both Docker containers that you build and those provided by SageMaker can send messages to the Stdout and Stderr files. SageMaker sends these messages to Amazon CloudWatch logs in your AWS account.\\nFor more information about how to create SageMaker containers and how scripts are executed inside them, see the SageMaker Training Toolkit and SageMaker Inference Toolkit repositories on GitHub. They also provide lists of important environmental variables and the environmental variables provided by SageMaker containers.', metadata={'source': '..\\\\docs_loka_1doc\\\\sagemaker_documentation\\\\amazon-sagemaker-toolkits.md'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template =\"\"\"Use only the following pieces of context to answer the question at the end. \\\n",
    "Don't use other knowledge that you might know from before. \\\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "qa = langchain_utils.get_retrieval_qa(llm = llm,\n",
    "                                      vector_db = docsearch,\n",
    "                                      prompt = prompt)\n",
    "result2 = qa({\"query\":\"What is SageMaker?\"})\n",
    "print(result2[\"result\"]) \n",
    "print(result2[\"source_documents\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T13:39:25.607684900Z",
     "start_time": "2023-10-16T13:35:22.960273200Z"
    }
   },
   "id": "ee4634f3a7bebcd0"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Amazon SageMaker is a fully managed platform to build, train, and deploy machine learning (ML) models quickly and at scale.\n",
      "\n",
      "[Document(page_content='/opt/ml\\n├── input\\n│   ├── config\\n│   │   ├── hyperparameters.json\\n│   │   └── resourceConfig.json\\n│   └── data\\n│       └── <channel_name>\\n│           └── <input data>\\n├── model\\n│\\n├── code\\n│\\n├── output\\n│\\n└── failure\\nWhen you run a model training job, the SageMaker container uses the /opt/ml/input/ directory, which contains the JSON files that configure the hyperparameters for the algorithm and the network layout used for distributed training. The /opt/ml/input/ directory also contains files that specify the channels through which SageMaker accesses the data, which is stored in Amazon Simple Storage Service (Amazon S3). The SageMaker containers library places the scripts that the container will run in the /opt/ml/code/ directory. Your script should write the model generated by your algorithm to the /opt/ml/model/ directory. For more information, see Use Your Own Training Algorithms.', metadata={'source': '..\\\\docs_loka_1doc\\\\sagemaker_documentation\\\\amazon-sagemaker-toolkits.md'}), Document(page_content='When you host a trained model on SageMaker to make inferences, you deploy the model to an HTTP endpoint. The model makes real-time predictions in response to inference requests. The container must contain a serving stack to process these requests.\\nIn a hosting or batch transform container, the model files are located in the same folder to which they were written during training.\\n/opt/ml/model\\n│\\n└── <model files>\\nFor more information, see Use your own inference code.\\nSingle Versus Multiple Containers\\nYou can either provide separate Docker images for the training algorithm and inference code or you can use a single Docker image for both. When creating Docker images for use with SageMaker, consider the following:\\n+ Providing two Docker images can increase storage requirements and cost because common libraries might be duplicated.', metadata={'source': '..\\\\docs_loka_1doc\\\\sagemaker_documentation\\\\amazon-sagemaker-toolkits.md'}), Document(page_content='+ Providing two Docker images can increase storage requirements and cost because common libraries might be duplicated.\\n+ In general, smaller containers start faster for both training and hosting. Models train faster and the hosting service can react to increases in traffic by automatically scaling more quickly.\\n+ You might be able to write an inference container that is significantly smaller than the training container. This is especially common when you use GPUs for training, but your inference code is optimized for CPUs.\\n+ SageMaker requires that Docker containers run without privileged access.\\n+ Both Docker containers that you build and those provided by SageMaker can send messages to the Stdout and Stderr files. SageMaker sends these messages to Amazon CloudWatch logs in your AWS account.', metadata={'source': '..\\\\docs_loka_1doc\\\\sagemaker_documentation\\\\amazon-sagemaker-toolkits.md'}), Document(page_content='+ Both Docker containers that you build and those provided by SageMaker can send messages to the Stdout and Stderr files. SageMaker sends these messages to Amazon CloudWatch logs in your AWS account.\\nFor more information about how to create SageMaker containers and how scripts are executed inside them, see the SageMaker Training Toolkit and SageMaker Inference Toolkit repositories on GitHub. They also provide lists of important environmental variables and the environmental variables provided by SageMaker containers.', metadata={'source': '..\\\\docs_loka_1doc\\\\sagemaker_documentation\\\\amazon-sagemaker-toolkits.md'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template =\"\"\"Use only the following pieces of context to answer the question at the end. \\\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "qa = langchain_utils.get_retrieval_qa(llm = llm,\n",
    "                                      vector_db = docsearch,\n",
    "                                      prompt = prompt)\n",
    "result2 = qa({\"query\":\"What is SageMaker?\"})\n",
    "print(result2[\"result\"]) \n",
    "print(result2[\"source_documents\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T14:08:20.256952900Z",
     "start_time": "2023-10-16T14:03:28.923320700Z"
    }
   },
   "id": "b5320d2b334d30ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "de2c52f14d04d84f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
